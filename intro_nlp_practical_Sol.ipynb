{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renadoo1/teaching/blob/main/intro_nlp_practical_Sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCUw45mZKjG"
      },
      "source": [
        "Intro to NLP Practical<br>\n",
        "======================<br>\n",
        "Students will work through problems on n-grams, probabilities, OOV handling, and classifiers.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l8ykuU-VuiD",
        "outputId": "09b7eb6a-b4d7-4810-a2f5-cede20e28352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.9.18)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Installing collected packages: joblib, click, nltk\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [nltk]]\u001b[33m  WARNING: The script nltk is installed in '/Library/Frameworks/Python.framework/Versions/3.13/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [nltk]\n",
            "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.1\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Downloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
            "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install nltk\n",
        "!python3 -m pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJnSQ_vqZKjO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2tr5e7WZKjQ"
      },
      "source": [
        "Toy corpus for language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uJmCM0sZKjS"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"Its fleece was white as snow\",\n",
        "    \"And everywhere that Mary went\",\n",
        "    \"The lamb was sure to go\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIvCzEeZKjT"
      },
      "source": [
        "--- Part 1: Preprocessing ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-DkrvQZKjU"
      },
      "source": [
        " Q1.1 Sequence notation<br>\n",
        "Exercise: Write sequence notation for the sentence:<br>\n",
        "\"Mary had a little lamb, its fleece was white as snow\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RRoQuOJVuiG",
        "outputId": "6599510b-6454-4a7c-8985-5ca22bac7f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence Notation:\n",
            "['x_1=Mary', 'x_2=had', 'x_3=a', 'x_4=little', 'x_5=lamb', 'x_6=its', 'x_7=fleece', 'x_8=was', 'x_9=white', 'x_10=as', 'x_11=snow']\n",
            "\n",
            "Indexed words:\n",
            "index 1: Mary\n",
            "index 2: had\n",
            "index 3: a\n",
            "index 4: little\n",
            "index 5: lamb\n",
            "index 6: its\n",
            "index 7: fleece\n",
            "index 8: was\n",
            "index 9: white\n",
            "index 10: as\n",
            "index 11: snow\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sentence = \"Mary had a little lamb, its fleece was white as snow\"\n",
        "\n",
        "# Remove commas, split into words\n",
        "words = sentence.replace(',', '').split()\n",
        "\n",
        "# Sequence notation\n",
        "sequence_notation = [f\"x_{i+1}={word}\" for i, word in enumerate(words)]\n",
        "print(\"Sequence Notation:\")\n",
        "print(sequence_notation)\n",
        "\n",
        "# Print the number-indexed words\n",
        "print(\"\\nIndexed words:\")\n",
        "for i, word in enumerate(words, start=1):\n",
        "    print(f\"index {i}: {word}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAOpzABuZKjV"
      },
      "source": [
        " Q1.2 Add start/end tokens<br>\n",
        "Exercise: Write a function to tokenize the corpus and add <s>, </s>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk9w753WVuiG",
        "outputId": "9a867a9f-59ff-4089-fcf0-c057c51c3e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Corpus with Start/End Tokens:\n",
            "[['<s>', 'Mary', 'had', 'a', 'little', 'lamb', '</s>'], ['<s>', 'Its', 'fleece', 'was', 'white', 'as', 'snow', '</s>'], ['<s>', 'And', 'everywhere', 'that', 'Mary', 'went', '</s>'], ['<s>', 'The', 'lamb', 'was', 'sure', 'to', 'go', '</s>']]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def add_start_end_tokens(corpus):\n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = ['<s>'] + sentence.split() + ['</s>']\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "    return tokenized_corpus\n",
        "\n",
        "print(\"Tokenized Corpus with Start/End Tokens:\")\n",
        "print(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH1mpV5ZKjY"
      },
      "source": [
        "--- Part 2: N-grams & Probabilities ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xVR2UlsZKjZ"
      },
      "source": [
        " Q2.1 Extract unigrams, bigrams, trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzjOb94TVuiH",
        "outputId": "6794f8a4-22cb-4a53-a9e1-243b2002c589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Unigrams:\n",
            "[('<s>',), ('Mary',), ('had',), ('a',), ('little',), ('lamb',), ('</s>',), ('<s>',), ('Its',), ('fleece',), ('was',), ('white',), ('as',), ('snow',), ('</s>',), ('<s>',), ('And',), ('everywhere',), ('that',), ('Mary',), ('went',), ('</s>',), ('<s>',), ('The',), ('lamb',), ('was',), ('sure',), ('to',), ('go',), ('</s>',)]\n",
            "\n",
            "Bigrams:\n",
            "[('<s>', 'Mary'), ('Mary', 'had'), ('had', 'a'), ('a', 'little'), ('little', 'lamb'), ('lamb', '</s>'), ('<s>', 'Its'), ('Its', 'fleece'), ('fleece', 'was'), ('was', 'white'), ('white', 'as'), ('as', 'snow'), ('snow', '</s>'), ('<s>', 'And'), ('And', 'everywhere'), ('everywhere', 'that'), ('that', 'Mary'), ('Mary', 'went'), ('went', '</s>'), ('<s>', 'The'), ('The', 'lamb'), ('lamb', 'was'), ('was', 'sure'), ('sure', 'to'), ('to', 'go'), ('go', '</s>')]\n",
            "\n",
            "Trigrams:\n",
            "[('<s>', 'Mary', 'had'), ('Mary', 'had', 'a'), ('had', 'a', 'little'), ('a', 'little', 'lamb'), ('little', 'lamb', '</s>'), ('<s>', 'Its', 'fleece'), ('Its', 'fleece', 'was'), ('fleece', 'was', 'white'), ('was', 'white', 'as'), ('white', 'as', 'snow'), ('as', 'snow', '</s>'), ('<s>', 'And', 'everywhere'), ('And', 'everywhere', 'that'), ('everywhere', 'that', 'Mary'), ('that', 'Mary', 'went'), ('Mary', 'went', '</s>'), ('<s>', 'The', 'lamb'), ('The', 'lamb', 'was'), ('lamb', 'was', 'sure'), ('was', 'sure', 'to'), ('sure', 'to', 'go'), ('to', 'go', '</s>')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def extract_ngrams(tokenized_corpus, n):\n",
        "    ngrams = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        ngrams.extend(zip(*[sentence[i:] for i in range(n)]))\n",
        "    return ngrams\n",
        "unigrams = extract_ngrams(tokenized_corpus, 1)\n",
        "bigrams = extract_ngrams(tokenized_corpus, 2)\n",
        "trigrams = extract_ngrams(tokenized_corpus, 3)\n",
        "print(\"\\nUnigrams:\")\n",
        "print(unigrams)\n",
        "print(\"\\nBigrams:\")\n",
        "print(bigrams)\n",
        "print(\"\\nTrigrams:\")\n",
        "print(trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuQzfXWZKjb"
      },
      "source": [
        " Q2.2 Bigram probabilities<br>\n",
        "Exercise: Write function to compute P(w_i | w_{i-1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb2oAt-kVuiI",
        "outputId": "96c41557-99fd-4a01-a433-a0fc5da74028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bigram Probabilities:\n",
            "P(Mary|<s>) = 0.2500\n",
            "P(had|Mary) = 0.5000\n",
            "P(a|had) = 1.0000\n",
            "P(little|a) = 1.0000\n",
            "P(lamb|little) = 1.0000\n",
            "P(</s>|lamb) = 0.5000\n",
            "P(Its|<s>) = 0.2500\n",
            "P(fleece|Its) = 1.0000\n",
            "P(was|fleece) = 1.0000\n",
            "P(white|was) = 0.5000\n",
            "P(as|white) = 1.0000\n",
            "P(snow|as) = 1.0000\n",
            "P(</s>|snow) = 1.0000\n",
            "P(And|<s>) = 0.2500\n",
            "P(everywhere|And) = 1.0000\n",
            "P(that|everywhere) = 1.0000\n",
            "P(Mary|that) = 1.0000\n",
            "P(went|Mary) = 0.5000\n",
            "P(</s>|went) = 1.0000\n",
            "P(The|<s>) = 0.2500\n",
            "P(lamb|The) = 1.0000\n",
            "P(was|lamb) = 0.5000\n",
            "P(sure|was) = 0.5000\n",
            "P(to|sure) = 1.0000\n",
            "P(go|to) = 1.0000\n",
            "P(</s>|go) = 1.0000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def bigram_probabilities(bigrams):\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    unigram_counts = Counter([bigram[0] for bigram in bigrams])\n",
        "    probabilities = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        probabilities[(w1, w2)] = count / unigram_counts[w1]\n",
        "    return probabilities\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "bigram_probs = bigram_probabilities(bigrams)\n",
        "for (w1, w2), prob in bigram_probs.items():\n",
        "    print(f\"P({w2}|{w1}) = {prob:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G1JGIAZKjj"
      },
      "source": [
        " Q2.3 Sentence probability<br>\n",
        "Exercise: Compute probability of \"Mary had a little lamb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLfXPnJ0VuiI",
        "outputId": "8f0f5105-f8a6-4ac0-ff52-f50c7be38d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probability of the sentence 'Mary had a little lamb': 0.0625000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def sentence_probability(sentence, bigram_probs):\n",
        "    words = ['<s>'] + sentence.replace(',', '').split() + ['</s>']\n",
        "    prob = 1.0\n",
        "    for i in range(1, len(words)):\n",
        "        bigram = (words[i-1], words[i])\n",
        "        prob *= bigram_probs.get(bigram, 0)\n",
        "    return prob\n",
        "sentence = \"Mary had a little lamb\"\n",
        "prob = sentence_probability(sentence, bigram_probs)\n",
        "print(f\"\\nProbability of the sentence '{sentence}': {prob:.10f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0unZilSZKjk"
      },
      "source": [
        "Q2.4 Handling OOV/UNK<br>\n",
        "Exercise: Replace unseen words with <UNK> and recompute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkI4pnRnVuiJ",
        "outputId": "5a267a29-fe8a-4834-ec7e-9c2b6f8ac4fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated Corpus with <UNK>:\n",
            "['Mary had a little lamb', 'Its fleece was white as snow', 'And everywhere that Mary went', 'The lamb was sure to go']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def replace_oov_with_unk(corpus, known_words):\n",
        "    updated_corpus = []\n",
        "    for sentence in corpus:\n",
        "        updated_sentence = ' '.join([word if word in known_words else '<UNK>' for word in sentence.split()])\n",
        "        updated_corpus.append(updated_sentence)\n",
        "    return updated_corpus\n",
        "known_words = set(word for sentence in tokenized_corpus for word in sentence)\n",
        "updated_corpus = replace_oov_with_unk(corpus, known_words)\n",
        "print(\"\\nUpdated Corpus with <UNK>:\")\n",
        "print(updated_corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evU5bff_ZKjl"
      },
      "source": [
        "--- Part 3: Classifier ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DxxnAKZKjm"
      },
      "source": [
        " Q3.1 Naive Bayes sentiment classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyx3z1RL58el"
      },
      "source": [
        "# 📽 Exercise 3.1: Sentiment Classification on toy dataset\n",
        "\n",
        "In this exercise, you will build a simple sentiment classification model that predicts whether a given sentence is **positive** or **negative**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "\n",
        "### 1️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3fkQ5DUZKjm"
      },
      "outputs": [],
      "source": [
        "train_texts = [\n",
        "    \"I love my dog\",\n",
        "    \"This food is great\",\n",
        "    \"I hate waiting\",\n",
        "    \"The movie was boring\",\n",
        "    \"Happy with my phone\",\n",
        "    \"This is awful\"\n",
        "]\n",
        "train_labels = [\"pos\", \"pos\", \"neg\", \"neg\", \"pos\", \"neg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq7Vj1ng1lw8"
      },
      "source": [
        "# 📽 Exercise 3.2: Movie Review Classification using Movies Review Corpus\n",
        "\n",
        "In this exercise, you will build a simple text classification model that predicts whether a given **movie review** is **positive** or **negative** using the **NLTK Movie Reviews Corpus**.\n",
        "\n",
        "This is a classical example of text classification at the **sentence level**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "### 1️⃣ Load the Data\n",
        "- Import the **Movie Reviews corpus** from **NLTK**.\n",
        "- Create a dataset where each example is a review and the label is either `'positive'` or `'negative'`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ Evaluate the Classifier\n",
        "- Use **accuracy** and a **classification report** to evaluate your model on the test set.\n",
        "- Think about: How well does the model perform? Which reviews are harder to classify?\n",
        "\n",
        "---\n",
        "\n",
        "✅ You are free to explore:\n",
        "- Trying different classifiers.\n",
        "- Visualizing the results (e.g., confusion matrix).\n",
        "\n",
        "---\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiBPScSvVuiK",
        "outputId": "1767f5ed-8369-45ef-9200-ab740900b864"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/renadalshamrani/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/renadalshamrani/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8YZeFBQVuiK",
        "outputId": "da0ac559-ddba-4c17-aef8-b54cee9b3435"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/renadalshamrani/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/renadalshamrani/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7900\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.78      0.81      0.80       202\n",
            "         pos       0.80      0.77      0.78       198\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# 1️⃣ Load the Data\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "documents = [(movie_reviews.raw(fileid), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "texts, labels = zip(*documents)\n",
        "texts, labels = list(texts), list(labels)\n",
        "\n",
        "# 2️⃣ Preprocess\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "texts = [preprocess(text) for text in texts]\n",
        "\n",
        "# 3️⃣ Train/Test Split\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4️⃣ Vectorize\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train_texts)\n",
        "X_test = vectorizer.transform(X_test_texts)\n",
        "\n",
        "# 5️⃣ Train Classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# 6️⃣ Evaluate\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYgDyUSVuiK"
      },
      "source": [
        "<Done By Best Group ever>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYgyaWtZKju"
      },
      "source": [
        " Q3.3 Discussion: Why bigrams vs unigrams?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mIB4S3wZKju"
      },
      "source": [
        "--- Part 4: Wrap-up Reflection ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv13VAlzZKju"
      },
      "source": [
        " Q3.4 Limitations of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzsTXZsTZKjv"
      },
      "source": [
        " Discussion Questions<br>\n",
        "1. Why do we need <UNK> tokens?<br>\n",
        "2. Why start/end tokens?<br>\n",
        "3. Why not always use higher n-grams?<br>\n",
        "4. How do classifiers differ from language models?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}